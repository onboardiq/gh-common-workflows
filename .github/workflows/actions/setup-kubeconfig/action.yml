name: 'Setup Kubeconfig'
description: 'Set up kubeconfig for a specific cluster'
inputs:
  aws-access-key-id:
    description: "An AWS Access Key ID with sufficient access to push to ECR repos"
    required: true
  aws-secret-access-key:
    description: "An AWS Secret Access Key matching the given AWS Access Key ID"
    required: true
  cloud:
    description: 'One of "aws" or "azure"'
    required: true
  region:
    description: 'An AWS or Azure region name'
    required: true
  stage:
    description: "One of contexts: 'stage' or 'prod'"
    required: true
  cluster-suffix:
    description: 'A string to differentiate between helm repos in the same region'
    default: ''

runs:
  using: 'composite'
  steps:
    - name: 'Add masks'
      shell: bash
      run: |
        echo "::add-mask::${{ inputs.aws-access-key-id }}";
        echo "::add-mask::${{ inputs.aws-secret-access-key }}";

    # GITHUB_RUN_ID is the unique identifier (id) within a repo
    - name: 'Set the KUBECONFIG'
      shell: bash
      run: |
        echo KUBECONFIG=${RUNNER_TEMP}/kubeconfig_${GITHUB_RUN_ID} >> $GITHUB_ENV

    - name: 'Deny azure'
      if: ${{ inputs.cloud == 'azure' }}
      shell: bash
      run: |
        echo "::error title=Azure not configured::Azure is not configured for setting up kubeconfig yet"
        
    - name: 'Get the kubeconfig'
      env:
        AWS_ACCESS_KEY_ID: ${{ inputs.aws-access-key-id }}
        AWS_SECRET_ACCESS_KEY: ${{ inputs.aws-secret-access-key }}
        AWS_DEFAULT_REGION: ${{ matrix.cluster.region }}
      shell: bash
      run: |
        aws eks update-kubeconfig \
        --name eng-${{ inputs.region }}-${{ inputs.stage }}-application-cluster${{ inputs.cluster-suffix }} \
        --alias aws-${{ inputs.region }}-${{ inputs.stage }} \
        --kubeconfig ${KUBECONFIG} \
        --region ${{ inputs.region }}

    
